{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c2e61ea-e84f-4a88-9c34-1200af5d4f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-core in /opt/anaconda3/lib/python3.12/site-packages (0.12.23.post2)\n",
      "Requirement already satisfied: llama-index-utils-workflow in /opt/anaconda3/lib/python3.12/site-packages (0.3.0)\n",
      "Requirement already satisfied: llama-index-llms-openai in /opt/anaconda3/lib/python3.12/site-packages (0.3.25)\n",
      "Requirement already satisfied: llama-parse in /opt/anaconda3/lib/python3.12/site-packages (0.6.4.post1)\n",
      "Requirement already satisfied: llama-index-embeddings-openai in /opt/anaconda3/lib/python3.12/site-packages (0.3.1)\n",
      "Requirement already satisfied: llama-index-readers-whisper in /opt/anaconda3/lib/python3.12/site-packages (0.1.0)\n",
      "Requirement already satisfied: gradio in /opt/anaconda3/lib/python3.12/site-packages (5.20.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (0.26.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (3.9.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (10.3.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (8.2.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.14.1)\n",
      "Requirement already satisfied: pyvis<0.4.0,>=0.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-utils-workflow) (0.3.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-llms-openai) (1.65.4)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.4 in /opt/anaconda3/lib/python3.12/site-packages (from llama-parse) (0.6.5)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.115.11)\n",
      "Requirement already satisfied: ffmpy in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.7.2)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.29.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (3.10.15)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pydub in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.9.10)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.46.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio-client==1.7.2->gradio) (15.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.9.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /opt/anaconda3/lib/python3.12/site-packages (from llama-cloud-services>=0.6.4->llama-parse) (8.1.7)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.14 in /opt/anaconda3/lib/python3.12/site-packages (from llama-cloud-services>=0.6.4->llama-parse) (0.1.14)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-cloud-services>=0.6.4->llama-parse) (1.0.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (2023.10.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (0.8.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core) (2.20.1)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (8.25.0)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (13.3.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json->llama-index-core) (3.21.3)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.8.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.5)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-core llama-index-utils-workflow llama-index-llms-openai \\\n",
    "    llama-parse llama-index-embeddings-openai llama-index-readers-whisper gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "621cf912-5fe4-4edb-9c85-4fd16f7dc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context\n",
    ")\n",
    "from helper import get_openai_api_key, get_llama_cloud_api_key\n",
    "from IPython.display import display, HTML\n",
    "from helper import extract_html_content\n",
    "from llama_index.utils.workflow import draw_all_possible_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb83f21a-ced8-4fe7-9e58-882495d94bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "064f60a5-b6a7-40f5-91ef-92edaf0e6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_cloud_api_key = get_llama_cloud_api_key()\n",
    "openai_api_key = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a323804-bed9-4a92-a68e-861e062dee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import InputRequiredEvent, HumanResponseEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2af1a14c-daac-4649-911a-c5b268fe3d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseFormEvent(Event):\n",
    "    application_form:str\n",
    "\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query:str\n",
    "    field:str\n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    response:str\n",
    "\n",
    "class FeedbackEvent(Event):\n",
    "    feedback:str\n",
    "\n",
    "class GenerateQuestionsEvent(Event):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfd4ccaf-8740-4ad7-a878-4a0ccae01b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "\n",
    "    storage_dir=\"./storage\"\n",
    "    llm= OpenAI\n",
    "    query_engine: VectorStoreIndex\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx:Context , ev:StartEvent) -> ParseFormEvent:\n",
    "\n",
    "        if not ev.resume_file:\n",
    "            raise valueError(\"No Resume File provided\")\n",
    "        if not ev.application_form:\n",
    "            raise valueError(\"No Application File provided\")\n",
    "        \n",
    "        self.llm= OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "        if os.path.exists(self.storage_dir):\n",
    "\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "            \n",
    "            index = load_index_from_storage(storage_context)\n",
    "            \n",
    "        else:\n",
    "            documents=LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "                result_type=\"markdown\",\n",
    "                content_guideline_instruction='This is a resume, gather related facts together and format it with bullet points with headers '\n",
    "            ).load_data(ev.resume_file)\n",
    "            #embed and index document \n",
    "\n",
    "            index=VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "\n",
    "\n",
    "        self.query_engine=index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        return ParseFormEvent(application_form)\n",
    "\n",
    "#form Parsing\n",
    "    @step\n",
    "    async def parse_form(self, ctx:Context, ev: ParseFormEvent)-> GenerateQuestionsEvent:\n",
    "        parser=LlamaParse(\n",
    "            api_key=llama_cloud_api_key,\n",
    "            base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=\"markdown\",\n",
    "            content_guideline_instruction=\"This is a job application form. Create a list of all the fields that need to be filled in.\",\n",
    "            formatting_instruction=\"Return a bulleted list of the fields ONLY.\"\n",
    "        )\n",
    "\n",
    "        #get LLM to convert parsed form into JSON \n",
    "        result = parser.load_data(ev.application_form)[0]\n",
    "        raw_json=self.llm.complete(\n",
    "            f\"This is a parsed form. Convert it into a JSON object containing only the list of fields to be filled in, in the form {{ fields: [...] }}. <form>{result.text}</form>. Return JSON ONLY, no markdown.\")\n",
    "        fields = json.loads(raw_json.text)[\"fields\"]\n",
    "\n",
    "        await ctx.set(\"fields_to_fill\",fields)\n",
    "\n",
    "        return GenerateQuestionsEvent()\n",
    "        \n",
    "    @step\n",
    "    async def generate_question(self,ctx:Context, ev:GenerateQuestionsEvent | FeedbackEvent) ->QueryEvent:\n",
    "#get the list of fields \n",
    "        fields =await ctx.get(\"fields to fill\")\n",
    "\n",
    "\n",
    "        for field in fields: \n",
    "            question =f\"How would you answer the question about the candidate?<field>{field}</field>\"\n",
    "\n",
    "\n",
    "        if hasattr(ev,\"feedback\"):\n",
    "            question += f\"\"\"\n",
    "                    \\n We previously got feedback about how we answered the questions.\n",
    "                    It might not be relevant to this particular field, but here it is:\n",
    "                    <feedback>{ev.feedback}</feedback>\n",
    "                \"\"\"\n",
    "\n",
    "        ctx.send_event(QueryEvent(\n",
    "            field=field,\n",
    "            query=question\n",
    "        ))\n",
    "\n",
    "        await ctx.set(\"total_fields\", len(fields))\n",
    "        return\n",
    "\n",
    "        @step\n",
    "        async def ask_question(self, ctx:Context, ev:QueryEvent)-> ResponseEvent:\n",
    "            response=self.query_engine.query(f\"this is a question about specific resume in our database:{ev.query}\")\n",
    "            return ResponseEvent (field=ev.field , response=response.response)\n",
    "\n",
    "\n",
    "        # Get feedback from the human\n",
    "    @step\n",
    "    async def fill_in_application(self, ctx: Context, ev: ResponseEvent) -> InputRequiredEvent:\n",
    "        # get the total number of fields to wait for\n",
    "        total_fields = await ctx.get(\"total_fields\")\n",
    "\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "        if responses is None:\n",
    "            return None # do nothing if there's nothing to do yet\n",
    "\n",
    "        # we've got all the responses!\n",
    "        responseList = \"\\n\".join(\"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses)\n",
    "\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You are given a list of fields in an application form and responses to\n",
    "            questions about those fields from a resume. Combine the two into a list of\n",
    "            fields and succinct, factual answers to fill in those fields.\n",
    "\n",
    "            <responses>\n",
    "            {responseList}\n",
    "            </responses>\n",
    "        \"\"\")\n",
    "\n",
    "        # save the result for later\n",
    "        await ctx.set(\"filled_form\", str(result))\n",
    "\n",
    "        # Fire off the feedback request\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"How does this look? Give me any feedback you have on any of the answers.\",\n",
    "            result=result\n",
    "        )\n",
    "\n",
    "\n",
    "        @step\n",
    "        # async def get_feedback(self, ctx:Context, ev: HumanResponseEvent)-> FeedbackEvent | StopEvent:\n",
    "        #         result=self.llm.complete(f\"\"\"\n",
    "        #     You have received some human feedback on the form-filling task you've done.\n",
    "        #     Does everything look good, or is there more work to be done?\n",
    "        #     <feedback>\n",
    "        #     {ev.response}\n",
    "        #     </feedback>\n",
    "        #     If everything is fine, respond with just the word 'OKAY'.\n",
    "        #     If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
    "        # \"\"\")\n",
    "\n",
    "        # verdict= result.text.strip()\n",
    "        # print(f\"LLM says the verdict was {verdict}\")\n",
    "        # if (verdict ==\"OKAY\"):\n",
    "        #     return StopEvent(result=await ctx.get(\"filed_form\"))\n",
    "        # else:\n",
    "        #     return FeedBackEvent(feedback=ev.repsonse)\n",
    "\n",
    "        async def get_feedback(self, ctx: Context, ev: HumanResponseEvent) -> FeedbackEvent | StopEvent:\n",
    "\n",
    "          result = self.llm.complete(f\"\"\"\n",
    "            You have received some human feedback on the form-filling task you've done.\n",
    "            Does everything look good, or is there more work to be done?\n",
    "            <feedback>\n",
    "            {ev.response}\n",
    "            </feedback>\n",
    "            If everything is fine, respond with just the word 'OKAY'.\n",
    "            If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
    "        \"\"\")\n",
    "\n",
    "        verdict = result.text.strip()\n",
    "\n",
    "        print(f\"LLM says the verdict was {verdict}\")\n",
    "        if (verdict == \"OKAY\"):\n",
    "            return StopEvent(result=await ctx.get(\"filled_form\"))\n",
    "        else:\n",
    "            return FeedbackEvent(feedback=ev.response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d5acf72-9df7-43ee-b1b3-ad5c00d06250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "\n",
    "    storage_dir = \"./storage\"\n",
    "    llm: OpenAI\n",
    "    query_engine: VectorStoreIndex\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "\n",
    "        if not ev.application_form:\n",
    "            raise ValueError(\"No application form provided\")\n",
    "\n",
    "        # define the LLM to work with\n",
    "        self.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "        # ingest the data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # you've already ingested the resume document\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=\n",
    "                                                           self.storage_dir)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        else:\n",
    "            # parse and load the resume document\n",
    "            documents = LlamaParse(\n",
    "                api_key=llama_cloud_api_key,\n",
    "                base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "                result_type=\"markdown\",\n",
    "                content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
    "            ).load_data(ev.resume_file)\n",
    "            \n",
    "# Debugging: Print parsed resume text\n",
    "            print(\"✅ Parsed Resume Text:\", documents[0].text if documents else \"❌ No text extracted from resume\")\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "            print(f\"✅ Indexed {len(index.docstore.docs)} documents\")\n",
    "\n",
    "        # create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        # let's pass the application form to a new step to parse it\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "\n",
    "    # form parsing\n",
    "    @step\n",
    "    async def parse_form(self, ctx: Context, ev: ParseFormEvent) -> GenerateQuestionsEvent:\n",
    "        parser = LlamaParse(\n",
    "            api_key=llama_cloud_api_key,\n",
    "            base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=\"markdown\",\n",
    "            content_guideline_instruction=\"This is a job application form. Create a list of all the fields that need to be filled in.\",\n",
    "            formatting_instruction=\"Return a bulleted list of the fields ONLY.\"\n",
    "        )\n",
    "        # data = parser.load_data(ev.application_form)\n",
    "        # print(\"Parsed Data:\", data)  # ✅ Debugging line to check if data exists\n",
    "\n",
    "        # if not data:\n",
    "        #     raise ValueError(\"Error: LlamaParse returned an empty list. Please check if the application form is valid.\")\n",
    "        # result = data[0]  # This will now only execute if data is not empty\n",
    "\n",
    "        # get the LLM to convert the parsed form into JSON\n",
    "        result = parser.load_data(ev.application_form)[0]\n",
    "        raw_json = self.llm.complete(\n",
    "            f\"This is a parsed form. Convert it into a JSON object containing only the list of fields to be filled in, in the form {{ fields: [...] }}. <form>{result.text}</form>. Return JSON ONLY, no markdown.\")\n",
    "        fields = json.loads(raw_json.text)[\"fields\"]\n",
    "\n",
    "        await ctx.set(\"fields_to_fill\", fields)\n",
    "\n",
    "        return GenerateQuestionsEvent()\n",
    "\n",
    "    # generate questions\n",
    "    @step\n",
    "    async def generate_questions(self, ctx: Context, ev: GenerateQuestionsEvent | FeedbackEvent) -> QueryEvent:\n",
    "\n",
    "        # get the list of fields to fill in\n",
    "        fields = await ctx.get(\"fields_to_fill\")\n",
    "\n",
    "        # generate one query for each of the fields, and fire them off\n",
    "        for field in fields:\n",
    "            question = f\"How would you answer this question about the candidate? <field>{field}</field>\"\n",
    "\n",
    "            # new! Is there feedback? If so, add it to the query:\n",
    "            if hasattr(ev,\"feedback\"):\n",
    "                question += f\"\"\"\n",
    "                    \\nWe previously got feedback about how we answered the questions.\n",
    "                    It might not be relevant to this particular field, but here it is:\n",
    "                    <feedback>{ev.feedback}</feedback>\n",
    "                \"\"\"\n",
    "\n",
    "            ctx.send_event(QueryEvent(\n",
    "                field=field,\n",
    "                query=question\n",
    "            ))\n",
    "\n",
    "        # store the number of fields so we know how many to wait for later\n",
    "        await ctx.set(\"total_fields\", len(fields))\n",
    "        return\n",
    "\n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = self.query_engine.query(f\"This is a question about the specific resume we have in our database: {ev.query}\")\n",
    "        return ResponseEvent(field=ev.field, response=response.response)\n",
    "\n",
    "\n",
    "    # Get feedback from the human\n",
    "    @step\n",
    "    async def fill_in_application(self, ctx: Context, ev: ResponseEvent) -> InputRequiredEvent:\n",
    "        # get the total number of fields to wait for\n",
    "        total_fields = await ctx.get(\"total_fields\")\n",
    "\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "        if responses is None:\n",
    "            return None # do nothing if there's nothing to do yet\n",
    "\n",
    "        # we've got all the responses!\n",
    "        responseList = \"\\n\".join(\"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses)\n",
    "\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You are given a list of fields in an application form and responses to\n",
    "            questions about those fields from a resume. Combine the two into a list of\n",
    "            fields and succinct, factual answers to fill in those fields.\n",
    "\n",
    "            <responses>\n",
    "            {responseList}\n",
    "            </responses>\n",
    "        \"\"\")\n",
    "\n",
    "        # save the result for later\n",
    "        await ctx.set(\"filled_form\", str(result))\n",
    "\n",
    "        # Fire off the feedback request\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"How does this look? Give me any feedback you have on any of the answers.\",\n",
    "            result=result\n",
    "        )\n",
    "\n",
    "    # Accept the feedback when a HumanResponseEvent fires\n",
    "    @step\n",
    "    async def get_feedback(self, ctx: Context, ev: HumanResponseEvent) -> FeedbackEvent | StopEvent:\n",
    "\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You have received some human feedback on the form-filling task you've done.\n",
    "            Does everything look good, or is there more work to be done?\n",
    "            <feedback>\n",
    "            {ev.response}\n",
    "            </feedback>\n",
    "            If everything is fine, respond with just the word 'OKAY'.\n",
    "            If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
    "        \"\"\")\n",
    "\n",
    "        verdict = result.text.strip()\n",
    "\n",
    "        print(f\"LLM says the verdict was {verdict}\")\n",
    "        if (verdict == \"OKAY\"):\n",
    "            return StopEvent(result=await ctx.get(\"filled_form\"))\n",
    "        else:\n",
    "            return FeedbackEvent(feedback=ev.response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d739fc2b-a83a-4c15-92fe-4524f7e33a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "WARNING: formatting_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id 44cd4317-fe25-4f23-9fee-57e38655bcf1\n",
      "We've filled in your form! Here are the results:\n",
      "\n",
      "<filled_fields>\n",
      "First Name: [No response provided]\n",
      "Last Name: [No response provided]\n",
      "Email: [No response provided]\n",
      "Phone: [No response provided]\n",
      "Linkedin: [No response provided]\n",
      "Project Portfolio: [No response provided]\n",
      "Degree: [No response provided]\n",
      "Graduation Date: [No response provided]\n",
      "Current Job Title: [No response provided]\n",
      "Current Employer: [No response provided]\n",
      "Technical Skills: [No response provided]\n",
      "Describe why you’re a good fit for this position: [No response provided]\n",
      "Do you have 5 years of experience in React?: [No response provided]\n",
      "</filled_fields>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How does this look? Give me any feedback you have on any of the answers. ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM says the verdict was OKAY\n",
      "Agent complete! Here's your final result:\n",
      "<filled_fields>\n",
      "First Name: [No response provided]\n",
      "Last Name: [No response provided]\n",
      "Email: [No response provided]\n",
      "Phone: [No response provided]\n",
      "Linkedin: [No response provided]\n",
      "Project Portfolio: [No response provided]\n",
      "Degree: [No response provided]\n",
      "Graduation Date: [No response provided]\n",
      "Current Job Title: [No response provided]\n",
      "Current Employer: [No response provided]\n",
      "Technical Skills: [No response provided]\n",
      "Describe why you’re a good fit for this position: [No response provided]\n",
      "Do you have 5 years of experience in React?: [No response provided]\n",
      "</filled_fields>\n"
     ]
    }
   ],
   "source": [
    "w = RAGWorkflow (timeout=600 , verbose=False)\n",
    "\n",
    "handler =w.run(\n",
    "    resume_file='/Users/saulehdeshmukh/Desktop/EventDrivenAgenticDocumentWorkflow/fake_resume.pdf',\n",
    "   application_form='/Users/saulehdeshmukh/Desktop/EventDrivenAgenticDocumentWorkflow/fake_application_form.pdf'\n",
    "    \n",
    "    )\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        print(\"We've filled in your form! Here are the results:\\n\")\n",
    "        print(event.result)\n",
    "        # now ask for input from the keyboard\n",
    "        response = input(event.prefix)\n",
    "        handler.ctx.send_event(\n",
    "            HumanResponseEvent(\n",
    "                response=response\n",
    "            )\n",
    "        )\n",
    "\n",
    "response = await handler\n",
    "print(\"Agent complete! Here's your final result:\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84cb49f2-57df-4e9a-b0aa-8c91729fec31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📄 Parsed resume:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdocuments[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext[:\u001b[38;5;241m1000\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"📄 Parsed resume:\\n{documents[0].text[:1000]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb274008-a40f-42a7-a986-2b47008f94c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Resume file path from event: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mev\u001b[38;5;241m.\u001b[39mresume_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ev' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"✅ Resume file path from event: {ev.resume_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bcb4ddf4-1cc9-42aa-811d-1060574579e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "    \n",
    "    storage_dir = \"./storage\"\n",
    "    llm: OpenAI\n",
    "    query_engine: VectorStoreIndex\n",
    "\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "\n",
    "        if not ev.application_form:\n",
    "            raise ValueError(\"No application form provided\")\n",
    "\n",
    "        # define the LLM to work with\n",
    "        self.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "        # ingest the data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # you've already ingested the resume document\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=\n",
    "                                                           self.storage_dir)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        # else:\n",
    "            # # parse and load the resume document\n",
    "            # documents = LlamaParse(\n",
    "            #     api_key=llama_cloud_api_key,\n",
    "            #     base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            #     result_type=\"markdown\",\n",
    "            #     content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
    "            # ).load_data(ev.resume_file)\n",
    "            # # embed and index the documents\n",
    "            # index = VectorStoreIndex.from_documents(\n",
    "            #     documents,\n",
    "            #     embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "            # )\n",
    "            # index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "\n",
    "        else:\n",
    "    # parse and load the resume document\n",
    "            documents = LlamaParse(\n",
    "            api_key=llama_cloud_api_key,\n",
    "            base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=\"markdown\",\n",
    "            content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
    "            ).load_data(ev.resume_file)\n",
    "    \n",
    "            print(\"✅ Parsed Resume Sample:\\n\", documents[0].text[:1000])  # 👈 Put it here\n",
    "        \n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "    \n",
    "\n",
    "        # create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        # let's pass the application form to a new step to parse it\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "\n",
    "    # form parsing\n",
    "    @step\n",
    "    async def parse_form(self, ctx: Context, ev: ParseFormEvent) -> GenerateQuestionsEvent:\n",
    "        parser = LlamaParse(\n",
    "            api_key=llama_cloud_api_key,\n",
    "            base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=\"markdown\",\n",
    "            content_guideline_instruction=\"This is a job application form. Create a list of all the fields that need to be filled in.\",\n",
    "            formatting_instruction=\"Return a bulleted list of the fields ONLY.\"\n",
    "        )\n",
    "\n",
    "        # get the LLM to convert the parsed form into JSON\n",
    "        result = parser.load_data(ev.application_form)[0]\n",
    "        raw_json = self.llm.complete(\n",
    "            f\"This is a parsed form. Convert it into a JSON object containing only the list of fields to be filled in, in the form {{ fields: [...] }}. <form>{result.text}</form>. Return JSON ONLY, no markdown.\")\n",
    "        fields = json.loads(raw_json.text)[\"fields\"]\n",
    "\n",
    "        await ctx.set(\"fields_to_fill\", fields)\n",
    "\n",
    "        return GenerateQuestionsEvent()\n",
    "\n",
    "    # generate questions\n",
    "    @step\n",
    "    async def generate_questions(self, ctx: Context, ev: GenerateQuestionsEvent | FeedbackEvent) -> QueryEvent:\n",
    "\n",
    "        # get the list of fields to fill in\n",
    "        fields = await ctx.get(\"fields_to_fill\")\n",
    "\n",
    "        # generate one query for each of the fields, and fire them off\n",
    "        for field in fields:\n",
    "            question = f\"How would you answer this question about the candidate? <field>{field}</field>\"\n",
    "\n",
    "            # new! Is there feedback? If so, add it to the query:\n",
    "            if hasattr(ev,\"feedback\"):\n",
    "                question += f\"\"\"\n",
    "                    \\nWe previously got feedback about how we answered the questions.\n",
    "                    It might not be relevant to this particular field, but here it is:\n",
    "                    <feedback>{ev.feedback}</feedback>\n",
    "                \"\"\"\n",
    "            \n",
    "            ctx.send_event(QueryEvent(\n",
    "                field=field,\n",
    "                query=question\n",
    "            ))\n",
    "\n",
    "        # store the number of fields so we know how many to wait for later\n",
    "        await ctx.set(\"total_fields\", len(fields))\n",
    "        return\n",
    "        \n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = self.query_engine.query(f\"This is a question about the specific resume we have in our database: {ev.query}\")\n",
    "        return ResponseEvent(field=ev.field, response=response.response)\n",
    "\n",
    "  \n",
    "    # Get feedback from the human\n",
    "    @step\n",
    "    async def fill_in_application(self, ctx: Context, ev: ResponseEvent) -> InputRequiredEvent:\n",
    "        # get the total number of fields to wait for\n",
    "        total_fields = await ctx.get(\"total_fields\")\n",
    "\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "        if responses is None:\n",
    "            return None # do nothing if there's nothing to do yet\n",
    "\n",
    "        # we've got all the responses!\n",
    "        responseList = \"\\n\".join(\"Field: \" + r.field + \"\\n\" + \"Response: \" + r.response for r in responses)\n",
    "\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You are given a list of fields in an application form and responses to\n",
    "            questions about those fields from a resume. Combine the two into a list of\n",
    "            fields and succinct, factual answers to fill in those fields.\n",
    "\n",
    "            <responses>\n",
    "            {responseList}\n",
    "            </responses>\n",
    "        \"\"\")\n",
    "\n",
    "        # save the result for later\n",
    "        await ctx.set(\"filled_form\", str(result))\n",
    "\n",
    "        # Fire off the feedback request\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"How does this look? Give me any feedback you have on any of the answers.\",\n",
    "            result=result\n",
    "        )\n",
    "\n",
    "    # Accept the feedback when a HumanResponseEvent fires\n",
    "    @step\n",
    "    async def get_feedback(self, ctx: Context, ev: HumanResponseEvent) -> FeedbackEvent | StopEvent:\n",
    "\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You have received some human feedback on the form-filling task you've done.\n",
    "            Does everything look good, or is there more work to be done?\n",
    "            <feedback>\n",
    "            {ev.response}\n",
    "            </feedback>\n",
    "            If everything is fine, respond with just the word 'OKAY'.\n",
    "            If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
    "        \"\"\")\n",
    "\n",
    "        verdict = result.text.strip()\n",
    "\n",
    "        print(f\"LLM says the verdict was {verdict}\")\n",
    "        if (verdict == \"OKAY\"):\n",
    "            return StopEvent(result=await ctx.get(\"filled_form\"))\n",
    "        else:\n",
    "            return FeedbackEvent(feedback=ev.response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "87a3777b-45d2-4719-8334-1f01dfe6a129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id 00bc9510-4619-4ff7-9d3e-e0cb4390c197\n",
      "✅ Parsed Resume Sample:\n",
      " # Sarah Chen\n",
      "\n",
      "Email: sarah.chen@email.com\n",
      "\n",
      "LinkedIn: linkedin.com/in/sarahchen\n",
      "\n",
      "GitHub: github.com/sarahcodes\n",
      "\n",
      "Portfolio: sarahchen.dev\n",
      "\n",
      "Location: San Francisco, CA\n",
      "\n",
      "# Professional Summary\n",
      "\n",
      "Innovative Full Stack Web Developer with 6+ years of experience crafting scalable web applications and microservices. Specialized in React, Node.js, and cloud architecture. Proven track record of leading technical teams and implementing CI/CD pipelines that reduced deployment time by 40%. Passionate about clean code, accessibility, and mentoring junior developers.\n",
      "\n",
      "# Professional Experience\n",
      "\n",
      "# Senior Full Stack Developer\n",
      "\n",
      "TechFlow Solutions | San Francisco, CA January 2022 - Present\n",
      "\n",
      "- Architected and implemented a microservices-based e-commerce platform serving 100K+ daily users\n",
      "- Led a team of 5 developers in rebuilding the company's flagship product using React and Node.js\n",
      "- Implemented GraphQL API gateway that reduced API response times by 60%\n",
      "- Established coding standards and review processes \n",
      "WARNING: content_guideline_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "WARNING: formatting_instruction is deprecated and may be remove in a future release. Use system_prompt, system_prompt_append or user_prompt instead.\n",
      "Started parsing the file under job_id 843aa8d8-774c-43de-a02f-7190fe4d37b0\n",
      "We've filled in your form! Here are the results:\n",
      "\n",
      "<Field: First Name\n",
      "Response: Sarah\n",
      "Field: Last Name\n",
      "Response: Chen\n",
      "Field: Email\n",
      "Response: sarah.chen@email.com\n",
      "Field: Phone\n",
      "Response: Not provided\n",
      "Field: Linkedin\n",
      "Response: linkedin.com/in/sarahchen\n",
      "Field: Project Portfolio\n",
      "Response: The candidate's project portfolio includes developing a carbon footprint tracking application called EcoTrack using React, Node.js, and MongoDB, which also featured a machine learning algorithm for personalized sustainability recommendations. Another project, ChatFlow, involved creating a real-time chat application with WebSocket protocol and React, serving over 5000 monthly active users. The candidate's projects showcase a range of technical skills and experiences in building web applications and services.\n",
      "Field: Degree\n",
      "Response: Bachelor of Science in Computer Science\n",
      "Field: Graduation Date\n",
      "Response: 2017\n",
      "Field: Current Job Title\n",
      "Response: Senior Full Stack Developer\n",
      "Field: Current Employer\n",
      "Response: TechFlow Solutions\n",
      "Field: Technical Skills\n",
      "Response: The candidate possesses strong technical skills in both frontend and backend development. On the frontend side, they are proficient in technologies such as React.js, Redux, Next.js, Vue.js, and various testing libraries. In backend development, they have experience with Node.js, Express.js, Python, Django, and working with databases like PostgreSQL and MongoDB. Additionally, the candidate is familiar with GraphQL, REST APIs, and has expertise in tools like Docker, Kubernetes, AWS services, and CI/CD pipelines.\n",
      "Field: Describe why you’re a good fit for this position\n",
      "Response: The candidate would likely highlight their 6+ years of experience as a Full Stack Web Developer, expertise in React, Node.js, and cloud architecture, leadership experience in leading technical teams, and implementing CI/CD pipelines to reduce deployment time. They may also mention their passion for clean code, accessibility, and mentoring junior developers as qualities that make them a strong fit for the position.\n",
      "Field: Do you have 5 years of experience in React?\n",
      "Response: Yes, the candidate has 6+ years of experience in React.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How does this look? Give me any feedback you have on any of the answers. ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM says the verdict was OKAY\n",
      "Agent complete! Here's your final result:\n",
      "<Field: First Name\n",
      "Response: Sarah\n",
      "Field: Last Name\n",
      "Response: Chen\n",
      "Field: Email\n",
      "Response: sarah.chen@email.com\n",
      "Field: Phone\n",
      "Response: Not provided\n",
      "Field: Linkedin\n",
      "Response: linkedin.com/in/sarahchen\n",
      "Field: Project Portfolio\n",
      "Response: The candidate's project portfolio includes developing a carbon footprint tracking application called EcoTrack using React, Node.js, and MongoDB, which also featured a machine learning algorithm for personalized sustainability recommendations. Another project, ChatFlow, involved creating a real-time chat application with WebSocket protocol and React, serving over 5000 monthly active users. The candidate's projects showcase a range of technical skills and experiences in building web applications and services.\n",
      "Field: Degree\n",
      "Response: Bachelor of Science in Computer Science\n",
      "Field: Graduation Date\n",
      "Response: 2017\n",
      "Field: Current Job Title\n",
      "Response: Senior Full Stack Developer\n",
      "Field: Current Employer\n",
      "Response: TechFlow Solutions\n",
      "Field: Technical Skills\n",
      "Response: The candidate possesses strong technical skills in both frontend and backend development. On the frontend side, they are proficient in technologies such as React.js, Redux, Next.js, Vue.js, and various testing libraries. In backend development, they have experience with Node.js, Express.js, Python, Django, and working with databases like PostgreSQL and MongoDB. Additionally, the candidate is familiar with GraphQL, REST APIs, and has expertise in tools like Docker, Kubernetes, AWS services, and CI/CD pipelines.\n",
      "Field: Describe why you’re a good fit for this position\n",
      "Response: The candidate would likely highlight their 6+ years of experience as a Full Stack Web Developer, expertise in React, Node.js, and cloud architecture, leadership experience in leading technical teams, and implementing CI/CD pipelines to reduce deployment time. They may also mention their passion for clean code, accessibility, and mentoring junior developers as qualities that make them a strong fit for the position.\n",
      "Field: Do you have 5 years of experience in React?\n",
      "Response: Yes, the candidate has 6+ years of experience in React.\n"
     ]
    }
   ],
   "source": [
    "w = RAGWorkflow (timeout=600 , verbose=False)\n",
    "\n",
    "handler =w.run(\n",
    "    resume_file='/Users/saulehdeshmukh/Desktop/EventDrivenAgenticDocumentWorkflow/fake_resume.pdf',\n",
    "   application_form='/Users/saulehdeshmukh/Desktop/EventDrivenAgenticDocumentWorkflow/fake_application_form.pdf'\n",
    "    \n",
    "    )\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        print(\"We've filled in your form! Here are the results:\\n\")\n",
    "        print(event.result)\n",
    "        # now ask for input from the keyboard\n",
    "        response = input(event.prefix)\n",
    "        handler.ctx.send_event(\n",
    "            HumanResponseEvent(\n",
    "                response=response\n",
    "            )\n",
    "        )\n",
    "\n",
    "response = await handler\n",
    "print(\"Agent complete! Here's your final result:\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab65de-f8d1-4eaf-af90-6bf1d96f6e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
